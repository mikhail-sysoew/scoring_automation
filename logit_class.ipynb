{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as sk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import time\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "from sklearn import cross_validation\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "warnings.simplefilter('ignore', DeprecationWarning)\n",
    "\n",
    "\n",
    "class Scorecard():\n",
    "    def __init__(self, max_bins=8, minimum_leaf=0.025, corr_threshold=0.8, odds_X_to_one = 100, odds_score=700, double_odds=25):        \n",
    "        self.regressor=LogisticRegression() #Regression build method\n",
    "        self.x = pd.DataFrame() #Input sample\n",
    "        self.y = pd.DataFrame() #Targets\n",
    "        self.vars = []\n",
    "        self.vars_after_iv_cut = []\n",
    "        self.vars_after_corr_cut = []\n",
    "        self.var_list_types = {} #Types of variables\n",
    "        self.var_list_bins = {} #Binning of scorecard variables dictionary\n",
    "        self.scorecard = pd.DataFrame() #Final scorecard representation\n",
    "        self.iv_table = {} #information value tables for each variable\n",
    "        self.gini = int #Gini of model \n",
    "        self.logit_model = [] #model object for LogisticRegression\n",
    "        self.max_bins = max_bins #Regularization parameter. Maximum bins used in decision tree\n",
    "        self.minimum_leaf = minimum_leaf #Regularization parameter. Mininmum size of one leaf\n",
    "        self.column = ''\n",
    "        self.iv_table = {} #Dictionary which contains iv table for each variable\n",
    "        self.x_one_hot = pd.DataFrame() #Input sample in one-hot view\n",
    "        self.corr_threshold = corr_threshold\n",
    "        self.odds_X_to_one = odds_X_to_one \n",
    "        self.odds_score = odds_score\n",
    "        self.double_odds = double_odds\n",
    "        self.x_binned = pd.DataFrame()\n",
    "        \n",
    "   \n",
    "  \n",
    "    #Learn model on sample\n",
    "    def fit(self,x,y,iv_treshold):        \n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.x = self.x.reset_index()\n",
    "        self.y = self.y.reset_index()\n",
    "        del self.x[\"index\"]\n",
    "        del self.y[\"index\"]      \n",
    "        self.fill_vars_cats()          \n",
    "        #print('Binning columns...')\n",
    "        #fill all values of var_list_bins\n",
    "        for col in self.x.columns: \n",
    "            #print(col)\n",
    "            self.binning(mode_forward='binning',mode_output='normal',column_name=col)  \n",
    "            #Filling IV table on current variable\n",
    "            df_t = pd.DataFrame(self.binning(mode_forward='forward',mode_output='normal',column_name=col))\n",
    "            df_t[\"y\"] = self.y\n",
    "            #df_t = df_t.rename(index=str, columns = {col:\"x\"})\n",
    "            df_iv =pd.DataFrame({'count': df_t.groupby(col)['y'].count(), \n",
    "                             'bad_rate': df_t.groupby(col)['y'].mean(),\n",
    "                             'total_goods': df_t.groupby(col)['y'].count() - df_t.groupby(col)['y'].sum(),\n",
    "                            'total_bads': df_t.groupby(col)['y'].sum() \n",
    "                             }).reset_index()\n",
    "            df_iv[\"cumm_bads\"] = df_iv['total_bads'].cumsum()\n",
    "            df_iv[\"cumm_goods\"] = df_iv['total_goods'].cumsum()\n",
    "            df_iv[\"cumm_total\"] = df_iv['count'].cumsum()\n",
    "            df_iv[\"per_bad\"] = df_iv[\"total_bads\"]/df_iv[\"cumm_bads\"].max()\n",
    "            df_iv[\"per_good\"] = df_iv[\"total_goods\"]/df_iv[\"cumm_goods\"].max()\n",
    "            df_iv[\"woe\"] = np.log((df_iv[\"per_good\"])/(df_iv[\"per_bad\"]+0.000000001))\n",
    "            iv = (df_iv[\"per_good\"] - df_iv[\"per_bad\"])*np.log((df_iv[\"per_good\"])/(df_iv[\"per_bad\"]+0.000000001))\n",
    "            df_iv[\"iv\"] = iv.sum()       \n",
    "            self.iv_table[col] = df_iv\n",
    "            if df_iv[\"iv\"].mean()>=iv_treshold: self.vars_after_iv_cut.append(col)\n",
    "        #creating sample in one-hot view\n",
    "        self.x_one_hot = pd.DataFrame(self.x.index.values)       \n",
    "        for col in self.vars_after_iv_cut:          \n",
    "            self.x_one_hot = pd.merge(self.x_one_hot, pd.DataFrame(self.binning(mode_forward='forward',mode_output='one-hot',column_name=col)),left_index=True,right_index=True)\n",
    "        del self.x_one_hot[self.x_one_hot.columns[0]]\n",
    "        self.x = self.x[self.vars_after_iv_cut] \n",
    "        #print('Exclude correlations...')\n",
    "        self.exclude_corr_factors()   \n",
    "        #print('Building regression...')\n",
    "        self.regressor.fit(self.x_one_hot,self.y)\n",
    "        self.scorecard_view()\n",
    "        \n",
    "    def predict_proba(self,x):\n",
    "        self.x = x        \n",
    "        self.x = x.reset_index()\n",
    "        del self.x[\"index\"]\n",
    "        self.x_binned = pd.DataFrame(self.x.index.values)\n",
    "        cols_to_delete = set(self.x.columns) - set(self.vars_after_iv_cut)\n",
    "        for c in cols_to_delete:\n",
    "            del self.x[c]\n",
    "        for col in self.x.columns:\n",
    "            self.x_binned = pd.merge(self.x_binned,pd.DataFrame(self.binning(mode_forward='forward',mode_output='one-hot',column_name = col)),left_index=True,right_index=True)\n",
    "            #del x_binned[x_binned.columns[0]]\n",
    "        cols_to_delete = set(self.x_binned.columns) - set(self.scorecard[\"Variable\"])\n",
    "        for c in cols_to_delete:\n",
    "            del self.x_binned[c]\n",
    "        return self.regressor.predict_proba(self.x_binned)[:,1]\n",
    "        \n",
    "    def predict_score(self,x):\n",
    "        y_pred = self.predict_proba(x)\n",
    "        bias = self.odds_score - self.double_odds*np.log(self.odds_X_to_one)/np.log(2)   \n",
    "        odds = self.double_odds/np.log(2)         \n",
    "        return bias+odds*np.log(1/y_pred-1)  \n",
    "      \n",
    "    \n",
    "    def scorecard_view(self):\n",
    "      #  print('Printing scorecard...')\n",
    "        self.scorecard=[]\n",
    "        cols = np.array('Intercept')\n",
    "        cols = np.append(cols,np.array(self.vars_after_corr_cut))\n",
    "        vals = np.array(self.regressor.intercept_)\n",
    "        vals = np.append(vals,np.array(self.regressor.coef_))\n",
    "        self.scorecard = pd.DataFrame(cols)\n",
    "        self.scorecard.rename(columns={0: 'Variable'},inplace=True)\n",
    "        self.scorecard[\"Regression_coef\"] = pd.DataFrame(vals)\n",
    "        b = self.double_odds/np.log(2)\n",
    "        a = self.odds_score - b*np.log(self.odds_X_to_one)    \n",
    "        self.scorecard[\"Score\"] = self.scorecard[\"Regression_coef\"]*b\n",
    "        self.scorecard[\"Score\"][0] = self.scorecard[\"Score\"][0]+a\n",
    "        self.scorecard[\"Score\"] = round(self.scorecard[\"Score\"],2)\n",
    "        \n",
    "    \n",
    "    \n",
    "    #Exclude correlations. Fill vars_after_corr_cut. Exclude correlated columns from x_one_hot\n",
    "    def exclude_corr_factors(self):\n",
    "        x_corr = self.x_one_hot.corr()\n",
    "        #Оставляем только колонки - потенциальные кандидаты на исключение (хотя бы одно значение корреляции выше трешхолда)\n",
    "        col_list=[]    \n",
    "        for i in range(len(x_corr.columns)):\n",
    "            #Заменяем диагональные значения на 0    \n",
    "            x_corr[x_corr.columns[i]][x_corr[x_corr.columns[i]].index.values[i]] = 0\n",
    "            #Если в колонке найдено, хотя бы одно значение с корреляцией больше трешхолда, добавляем ее в лист\n",
    "            if max(abs(x_corr[x_corr.columns[i]]))>self.corr_threshold: col_list.append(x_corr.columns[i])\n",
    "        #Оставляем только те колонки, из которых нужно выбрать которые выкинуть из-за корреляций            \n",
    "        x_dev_drop =  self.x_one_hot[col_list]\n",
    "        #Строим корреляционную матрицу из оставшихся\n",
    "        x_c = x_dev_drop.corr()\n",
    "        #Пустой список\n",
    "        corr_list = []\n",
    "        corr_list.append([])\n",
    "        exclude_iteration = 0\n",
    "        var_list = [0,1]\n",
    "        #Заполняем диагональ нулями\n",
    "        for i in range(len(x_c.columns)):        \n",
    "            x_c[x_c.columns[i]][x_c[x_c.columns[i]].index.values[i]] = 0\n",
    "        while len(var_list)>1&len(x_c)>0:\n",
    "            for i in range(len(x_c.columns)):        \n",
    "                x_c[x_c.columns[i]][x_c[x_c.columns[i]].index.values[i]] = 0\n",
    "            #Если нашли хотя бы одну колонку, которая коррелирует с первой, создаем пару в corr_list и записываем туда первую колонку\n",
    "            if max(abs(x_c[x_c.columns[0]]))>=self.corr_threshold:     \n",
    "                corr_list[exclude_iteration].append(x_c.columns[0])\n",
    "            #Пробегаемся по всем колонкам\n",
    "                for i in range(len(x_c.columns)):\n",
    "            #Записываем в пару к первой все коррелирующие с ней колонки\n",
    "                    if abs(x_c[x_c.columns[0]].iloc[i])>=self.corr_threshold:\n",
    "                        corr_list[exclude_iteration].append(x_c.columns[i])\n",
    "                #Выкидываем все колонки, которые коррелируют с первой\n",
    "                var_list = [x for x in x_c.columns.values if x not in corr_list[exclude_iteration]]\n",
    "                x_dev_drop = x_dev_drop[var_list]\n",
    "                x_c = x_dev_drop.corr()\n",
    "                corr_list.append([])\n",
    "                exclude_iteration = exclude_iteration+1\n",
    "                #print(\"Excluding correlations. Iteration = \",exclude_iteration,\"Corr list: \", corr_list)\n",
    "        #После обработки corr_list содержит все списки коррелирующих колонок. Из каждого списка оставляем только одну\n",
    "        cols_to_drop=[] #Список колонок, которые надо выкинуть\n",
    "        for i in range(len(corr_list)):\n",
    "            for j in range(len(corr_list[i])):\n",
    "                if j!=0: \n",
    "                    cols_to_drop.append(corr_list[i][j])\n",
    "        #Оставляем в исходном списке только колонки не из col_to_drop\n",
    "        exclude_list = [x for x in self.x_one_hot.columns.values if x not in cols_to_drop]\n",
    "        self.x_one_hot = self.x_one_hot[exclude_list]\n",
    "        self.vars_after_corr_cut = exclude_list\n",
    "        \n",
    "    \n",
    "    #Input - one variable name \n",
    "    #Output - optimal binning, builded on decision tree. Maximum number of bins = max_bins\n",
    "    def split_numeric(self,column_name):  \n",
    "        x_train_t = np.array(self.x[column_name][self.x[column_name].notnull()]) #Exclude nulls \n",
    "        y_train_t = np.array(self.y[self.x[column_name].notnull()])\n",
    "        x_train_t = x_train_t.reshape(x_train_t.shape[0], 1) #Need for DecisionTreeClassifier\n",
    "        m_depth = int(np.log2(self.max_bins)) + 1 #Maximum tree depth\n",
    "        bad_rate = y.mean()\n",
    "        start = 1\n",
    "        cv_scores = []\n",
    "        cv = 3\n",
    "        for i in range(start,m_depth): #Loop over all tree depth. CV on the each step\n",
    "            d_tree = tree.DecisionTreeClassifier(criterion='gini', max_depth=i, min_samples_leaf=self.minimum_leaf)\n",
    "            scores = cross_val_score(d_tree, x_train_t, y_train_t, cv=cv,scoring='roc_auc')   \n",
    "            cv_scores.append(scores.mean())        \n",
    "        best = np.argmax(cv_scores) + start #Criterion - maximum GINI on validation set        \n",
    "        final_tree = tree.DecisionTreeClassifier(criterion='gini', max_depth=best, min_samples_leaf=0.025) #Build final tree\n",
    "        final_tree.fit(x_train_t, y_train_t)\n",
    "        #Final tree\n",
    "        opt_bins = final_tree.tree_.threshold[final_tree.tree_.feature >= 0]        \n",
    "        opt_bins = np.append(opt_bins,max(x_train_t)+1)#Add right border\n",
    "        opt_bins = np.append(opt_bins,min(x_train_t)-1)#Add left border\n",
    "        opt_bins = np.sort(opt_bins)    \n",
    "        return opt_bins #Return optimal binning\n",
    "    \n",
    "    #Split categorial variable. Grouping variable for regularization.\n",
    "    #Input = column name\n",
    "    #Output : add to var_list_bins binned variable as dictionary\n",
    "    def split_categorial(self,column_name):\n",
    "        #One-hot encoding\n",
    "        self.x[column_name] = self.x[column_name].fillna('MISSING')\n",
    "        x_cat = pd.get_dummies(self.x[column_name],prefix = self.x[column_name].name)\n",
    "        bad_rate = self.y.mean()\n",
    "        max_bins = max(self.x[column_name].nunique(),20)\n",
    "        #Classification by decision tree\n",
    "        m_depth = max_bins+1\n",
    "        start = 1\n",
    "        cv_scores = []\n",
    "        cv = 3\n",
    "        for i in range(start,m_depth):\n",
    "            d_tree = tree.DecisionTreeClassifier(criterion='gini', max_depth=i, min_samples_leaf=self.minimum_leaf) \n",
    "            scores = cross_val_score(d_tree, x_cat, self.y, cv=cv,scoring='roc_auc') \n",
    "            cv_scores.append(scores.mean())\n",
    "        #    print(\"Number of bins = \", i,\"; GINI = \",2*scores.mean()-1)\n",
    "        best = np.argmax(cv_scores) + start #Choose maximizing GINI on validation dataset\n",
    "        #print(\"Optimal number of bins: \",best, \"; GINI = \",2*max(cv_scores)-1)\n",
    "        final_tree = tree.DecisionTreeClassifier(criterion='gini', max_depth=best, min_samples_leaf=0.025) #Build final tree\n",
    "        final_tree.fit(x_cat, self.y)\n",
    "\n",
    "        #Get leafes names\n",
    "        x_l = final_tree.apply(x_cat)\n",
    "        tmp = pd.DataFrame(self.x[column_name])\n",
    "        tmp[\"LEAF\"] = x_l\n",
    "\n",
    "        #Make dictionary with optimal binning\n",
    "        d = {}\n",
    "        for leaf in tmp[\"LEAF\"].unique():\n",
    "            d[leaf]=str(self.x[column_name][tmp[\"LEAF\"]==leaf].unique())   \n",
    "        tmp[\"x_num\"] = tmp[\"LEAF\"].apply(lambda x: d.get(x))\n",
    "        return d\n",
    "   \n",
    "    #Define variable category - numeric or categorial\n",
    "    #Input - column name\n",
    "    #Output - numeric or cat\n",
    "    def check_type(self,column_name):\n",
    "        from pandas.api.types import is_string_dtype\n",
    "        from pandas.api.types import is_numeric_dtype   \n",
    "        #delete nulls\n",
    "        tmp_var = self.x[column_name][self.x[column_name].notnull()]\n",
    "        #If number of uniques<=4 then type = categorial\n",
    "        if tmp_var.nunique()<=4: return 'cat'\n",
    "        elif is_numeric_dtype(tmp_var): return 'numeric'\n",
    "        else: return 'cat'\n",
    "    \n",
    "    #Fill variable var_list_cats\n",
    "    def fill_vars_cats(self):\n",
    "        from pandas.api.types import is_string_dtype\n",
    "        from pandas.api.types import is_numeric_dtype \n",
    "        for col in self.x[self.x.columns]:\n",
    "            if self.check_type(col)=='numeric': self.var_list_types[col]='numeric'\n",
    "            if self.check_type(col)=='cat': \n",
    "                self.var_list_types[col]='cat'\n",
    "                if (self.x[col].nunique()<=4)&(is_numeric_dtype(self.x[col])): self.x[col] = self.x[col].apply(lambda x: 'cat_'+str(x))\n",
    "                \n",
    "    \n",
    "    #Add leading zeros to names\n",
    "    def zero_pad(self,x):\n",
    "        if str(x)=='MISSING': return '000'\n",
    "        if len(str(x))==3: return str('00'+str(x))[:-2]+': '\n",
    "        if len(str(x))==4: return str('0'+str(x))[:-2]+': '\n",
    "        if len(str(x))==5: str(x)[:-2]+': '\n",
    "    \n",
    "    #Naming for categories by rank\n",
    "    def make_dict(x):        \n",
    "        x_dict = x.groupby(0)[\"val\"].min().fillna(0).sort_values().reset_index().rename(index=str, columns={0: \"x\"})\n",
    "        x_dict['rownum'] = x_dict['val'].rank(method='first', na_option='top')\n",
    "        x_dict['rownum'] = x_dict['rownum'].apply(zero_pad)\n",
    "        x_dict['x_num'] = x_dict[\"rownum\"].map(str)+x_dict[\"x\"].map(str)\n",
    "        del x_dict['val']\n",
    "        del x_dict['rownum']\n",
    "        return x_dict   \n",
    "    \n",
    "    #Binning procedure\n",
    "    #Return binned sample. Has two modes - one-hot and norma;\n",
    "    #Inputs \n",
    "    #      x - sample\n",
    "    #      y - targets\n",
    "    #      max_bins - maximum number of bins\n",
    "    #      optimal_bins - for mode_output = 'normal' or 'one-hot' using as input for feed forward\n",
    "    #                     for mode_forward='binning' calculating of optimal bins\n",
    "    #                         mode_forward='forward' calculating outputs using optimals bins as input \n",
    "    #\n",
    "    \n",
    "    #Need for feed forward categorial variables\n",
    "    #Take value from dictionary var_list_bins and answer if current value is in list\n",
    "    #If yes - return list\n",
    "    \n",
    "    \n",
    "    def forward_cat(self,x):\n",
    "        for i in self.var_list_bins[self.column].keys():\n",
    "            if str(x) in self.var_list_bins[self.column][i]:\n",
    "                return str(self.var_list_bins[self.column][i]) \n",
    "    \n",
    "    def binning(self,mode_output,mode_forward,column_name):\n",
    "        variable_type = self.var_list_types[column_name]\n",
    "        if (variable_type=='numeric')&(mode_forward=='forward'):         \n",
    "            #Вспомогательная переменная, хранящая разбиения по непустым значениям\n",
    "            x_bin_t = pd.cut(self.x[column_name][self.x[column_name].notnull()],bins=self.var_list_bins[column_name])    \n",
    "            #Вспомогательная переменная, хранящая one-hot по непустым значениям\n",
    "            x_bin = pd.get_dummies(x_bin_t,prefix=self.x[column_name].name,drop_first=True)\n",
    "            #Добавляем колонку с пустыми значениями\n",
    "            x_bin[self.x[column_name].name+'_ISNULL']=0\n",
    "            x_null = pd.DataFrame(self.x[column_name][self.x[column_name].isnull()])\n",
    "            for i in x_bin.columns:\n",
    "                x_null[i]=0\n",
    "            x_null[self.x[column_name].name+'_ISNULL']=1\n",
    "            del x_null[self.x[column_name].name]\n",
    "            #Если нет NULL то колонку с dummy is null удаляем   \n",
    "            if len(self.x[column_name][self.x[column_name].isnull()])==0:\n",
    "                del x_null[self.x[column_name].name+'_ISNULL']\n",
    "                del x_bin[self.x[column_name].name+'_ISNULL']\n",
    "            #Вспомогательная переменная, которая хранит узкий и широкий вид, включая пустые значения    \n",
    "            x_pivot = pd.concat([x_bin_t,pd.DataFrame(self.x[column_name][self.x[column_name].isnull()])]).sort_index(axis=0)        \n",
    "            del x_pivot[self.x[column_name].name]\n",
    "            #Заполняем пустые значения MISSING\n",
    "            x_pivot = x_pivot.fillna('MISSING')\n",
    "            x_pivot['val'] = self.x[column_name]        \n",
    "            #Добавляем категориям индекс (создается справочник)           \n",
    "            x_dict = x_pivot.groupby(0)[\"val\"].min().fillna(0).sort_values().reset_index().rename(index=str, columns={0: \"x\"})\n",
    "            x_dict['rownum'] = x_dict['val'].rank(method='first', na_option='top')\n",
    "            x_dict['rownum'] = x_dict['rownum'].apply(self.zero_pad)\n",
    "            x_dict[column_name] = x_dict[\"rownum\"].map(str)+x_dict[\"x\"].map(str)\n",
    "            del x_dict['val']\n",
    "            del x_dict['rownum']\n",
    "            x_d =  x_dict   \n",
    "            x_pivot[\"rownum\"] = x_pivot.index.values\n",
    "            x_pivot = pd.merge(x_pivot,x_d,left_on=0,right_on=\"x\").sort_values(by='rownum').reset_index()[column_name]\n",
    "            #Джойним значения со справочником, удаляем исходные        \n",
    "            if mode_output=='one-hot': return pd.concat([x_bin,x_null]).sort_index(axis=0) #Возвращаем в виде on-hot                            \n",
    "            if mode_output=='normal': return x_pivot #Возвращаем в \"длинном и узком\" виде               \n",
    "        if (variable_type=='cat')&(mode_forward=='forward'): \n",
    "            ####################INPUT CODE HERE#####################\n",
    "            if mode_output=='normal': \n",
    "                self.column = column_name\n",
    "                return self.x[column_name].apply(self.forward_cat)\n",
    "            if mode_output=='one-hot': \n",
    "                self.column = column_name\n",
    "                return pd.get_dummies(self.x[column_name].apply(self.forward_cat), drop_first=True,prefix=self.x[column_name].name)\n",
    "        if (variable_type=='numeric')&(mode_forward=='binning'):\n",
    "            self.var_list_bins[column_name] = self.split_numeric(column_name)\n",
    "        if (variable_type=='cat')&(mode_forward=='binning'):                \n",
    "            self.var_list_bins[column_name] = self.split_categorial(column_name)\n",
    "            #x_bin = self.split_categorial(column_name)          \n",
    "            #if mode_output=='one-hot': return pd.get_dummies(x_bin,prefix=self.x[column_name].name,drop_first=True)\n",
    "            #if mode_output=='normal': return pd.DataFrame(x_bin)\n",
    "    \n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "df = pd.read_csv('./data/cc_sample.txt', sep=';',index_col=False, decimal=',') \n",
    "x = df.copy()\n",
    "x = x[x['BAD_12_FLAG90'].notnull()]\n",
    "y = x['BAD_12_FLAG90']\n",
    "x = x.drop(['CONTRACT_SRC_CODE','SCORE_FINAL','BAD_12_FLAG90'],axis=1)\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y)\n",
    "\n",
    "for max_bin in (2,4,8,16,32):\n",
    "    s = Scorecard(max_bins = max_bin, corr_threshold=0.8)\n",
    "    s.fit(x_train,y_train,iv_treshold=0.05)\n",
    "    print(max_bin)\n",
    "    print('Train: ', 2*roc_auc_score(y_train,s.predict_proba(x_train)))\n",
    "    print('Test: ', 2*roc_auc_score(y_test,s.predict_proba(x_test))    )  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
