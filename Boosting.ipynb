{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as sk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import time\n",
    "from sklearn.model_selection import KFold\n",
    "import warnings\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from hyperopt import hp\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7515923976898193  sec\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "df = pd.read_csv('./data/cc_sample.txt', sep=';',index_col=False, decimal=',') \n",
    "print(-start_time + time.time(),\" sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions with xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Dataframe as input, return DataFrame with filled nulls\n",
    "def fill_null(x,s):\n",
    "    for i in x.columns:\n",
    "        x[i] = x[i].fillna(s)\n",
    "    return x\n",
    "\n",
    "#Check type of variable and return numeric or cat\n",
    "def check_type(x):\n",
    "    from pandas.api.types import is_string_dtype\n",
    "    from pandas.api.types import is_numeric_dtype   \n",
    "    #Удаляем пустые значения\n",
    "    x = x[x.notnull()]\n",
    "    #Если число различных значений меньше 4, то тип-категориальный\n",
    "    if x.nunique()<=4: return 'cat'\n",
    "    elif is_numeric_dtype(x): return 'numeric'\n",
    "    else: return 'cat'\n",
    "    \n",
    "#Input: DataFrame\n",
    "#Output: DataFrame with one-hot variables\n",
    "def cat_to_one_hot(x):\n",
    "    for col in x.columns:\n",
    "        if check_type(x[col])=='cat':\n",
    "            tmp = pd.get_dummies(x[col],prefix=x[col].name,drop_first=True)\n",
    "            for i in tmp.columns:\n",
    "                x[i] = tmp[i]\n",
    "            del x[col]\n",
    "    return x\n",
    "\n",
    "def gini(model,x,y):\n",
    "    gini =  2*roc_auc_score(y,model.predict_proba(x)[:,1])-1\n",
    "   # print('GINI = ',2*roc_auc_score(y,model.predict_proba(x)[:,1])-1)   \n",
    "    return gini\n",
    "\n",
    "#Searching for optimal hyperparams using gridsearch\n",
    "def gridcv_xgboost(params_grid,x,y):\n",
    "    xgb_model = xgb.XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
    "                               gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=4,\n",
    "                               min_child_weight=1, missing=None, n_estimators=500, nthread=-1,\n",
    "                               objective='binary:logistic', reg_alpha=30, reg_lambda=1,\n",
    "                               scale_pos_weight=1, seed=42, silent=True, subsample=1,tree_method='gpu_hist')\n",
    "    kfold = KFold(n_splits=3, shuffle=True)\n",
    "    clf = GridSearchCV(xgb_model,params_grid, \n",
    "                            verbose=2, \n",
    "                            scoring='roc_auc',\n",
    "                            cv=kfold,\n",
    "                            n_jobs=-1)\n",
    "    clf.fit(x,y)\n",
    "    print(-start_time + time.time(),\" sec\")\n",
    "    warnings.simplefilter('ignore', FutureWarning)\n",
    "    res = pd.DataFrame(clf.cv_results_)\n",
    "    res[\"dev_gini\"] = 2*res[\"mean_train_score\"]-1\n",
    "    res[\"val_gini\"] = 2*res[\"mean_test_score\"]-1\n",
    "    return res[[\"params\",\"dev_gini\",\"val_gini\"]]\n",
    "\n",
    "#Build single xgoost model using params\n",
    "def xgb_build(params):\n",
    "    print(\"Training with params: \")\n",
    "    print(params)\n",
    "    num_round = int(params['n_estimators'])\n",
    "    del params['n_estimators']\n",
    "    gbm_model = xgb.XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
    "                               gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=4,\n",
    "                               min_child_weight=1, missing=None, n_estimators=500, nthread=-1,\n",
    "                               objective='binary:logistic', reg_alpha=30, reg_lambda=1,\n",
    "                               scale_pos_weight=1, seed=42, silent=True, subsample=1)\n",
    "    gbm_model.fit(x_train,y_train)\n",
    "    predictions = gbm_model.predict(x_valid)\n",
    "    score = roc_auc_score(y_valid, predictions)\n",
    "    # TODO: Add the importance for the selected features\n",
    "    print(\"\\tScore {0}\\n\\n\".format(score))\n",
    "    # The score function should return the loss (1-score)\n",
    "    # since the optimize function looks for the minimum\n",
    "    gini = 2*score-1\n",
    "    return {'gini': gini, 'status': STATUS_OK}\n",
    "\n",
    "#Find optimal params using hyperopt\n",
    "def optimize(\n",
    "             #trials, \n",
    "             random_state=1):\n",
    "    \"\"\"\n",
    "    This is the optimization function that given a space (space here) of \n",
    "    hyperparameters and a scoring function (score here), finds the best hyperparameters.\n",
    "    \"\"\"\n",
    "    # To learn more about XGBoost parameters, head to this page: \n",
    "    # https://github.com/dmlc/xgboost/blob/master/doc/parameter.md\n",
    "    space = {\n",
    "        'n_estimators': hp.quniform('n_estimators', 50,100, 1),\n",
    "        'learning_rate': hp.quniform('learning_rate', 0.08,0.12, 0.01),\n",
    "        'eta': hp.quniform('eta', 0.025, 0.5, 0.025),\n",
    "        # A problem with max_depth casted to float instead of int with\n",
    "        # the hp.quniform method.\n",
    "        'max_depth':  hp.choice('max_depth', np.arange(3, 4, dtype=int)),\n",
    "        'min_child_weight': hp.quniform('min_child_weight', 1, 6, 1),\n",
    "        'subsample': hp.quniform('subsample', 0.5, 1, 0.05),\n",
    "        'gamma': hp.quniform('gamma', 0.5, 1, 0.05),\n",
    "        'colsample_bytree': hp.quniform('colsample_bytree', 0.5, 1, 0.05),\n",
    "        'eval_metric': 'auc',\n",
    "        'objective': 'binary:logistic',\n",
    "        # Increase this number if you have more cores. Otherwise, remove it and it will default \n",
    "        # to the maxium number. \n",
    "        'nthread': 4,\n",
    "        'booster': 'gbtree',\n",
    "        'tree_method': 'exact',\n",
    "        'silent': 1,\n",
    "        'seed': random_state\n",
    "    }\n",
    "    # Use the fmin function from Hyperopt to find the best hyperparameters\n",
    "    best = fmin(xgb_build, space, algo=tpe.suggest, \n",
    "                # trials=trials, \n",
    "                max_evals=250)\n",
    "    return best\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data and applying xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7091481685638428  sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=2, min_child_weight=1, missing=None, n_estimators=2,\n",
       "       n_jobs=1, nthread=-1, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=30, reg_lambda=1, scale_pos_weight=1, seed=42,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#XGBoost with GridSearch and CV\n",
    "start_time = time.time()\n",
    "df = pd.read_csv('./data/cc_sample.txt', sep=';',index_col=False, decimal=',') \n",
    "print(-start_time + time.time(),\" sec\")\n",
    "\n",
    "df = fill_null(df,-1)\n",
    "df = cat_to_one_hot(df)\n",
    "start_time = time.time()\n",
    "x=df.copy()\n",
    "x= x.drop(['CONTRACT_SRC_CODE','SCORE_FINAL','BAD_12_FLAG90_1'], axis=1)\n",
    "y = df[\"BAD_12_FLAG90_1\"][df['BAD_12_FLAG90_1'].notnull()] \n",
    "\n",
    "params_grid = {'n_estimators': [500],\n",
    "              'max_depth':[3],\n",
    "              'learning_rate':[0.1],\n",
    "              'reg_alpha':[30]}\n",
    "\n",
    "#gridcv_xgboost(params_grid,x_sample,y)\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y)\n",
    "gbm_model = xgb.XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
    "                               gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=2,\n",
    "                               min_child_weight=1, missing=None, n_estimators=2, nthread=-1,\n",
    "                               objective='binary:logistic', reg_alpha=30, reg_lambda=1,\n",
    "                               scale_pos_weight=1, seed=42, silent=True, subsample=1)\n",
    "gbm_model.fit(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model to file and view model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "booster[0]:\n",
      "0:[LBT_ACCT_TOT_BAL_PREV_RUB_AMT<30.5649986] yes=1,no=2,missing=1,gain=1094.875,cover=157760.25\n",
      "\t1:[MIN_APP_DAYS<0.5] yes=3,no=4,missing=3,gain=161.640625,cover=14666.75\n",
      "\t\t3:leaf=-0.145004302,cover=8156\n",
      "\t\t4:leaf=-0.174092993,cover=6510.75\n",
      "\t2:[CNT_TR_CASH_1M<16.5] yes=5,no=6,missing=5,gain=38.21875,cover=143093.5\n",
      "\t\t5:leaf=-0.18950969,cover=140195\n",
      "\t\t6:leaf=-0.162200376,cover=2898.5\n",
      "booster[1]:\n",
      "0:[LBT_ACCT_TOT_BAL_PREV_RUB_AMT<64.0350037] yes=1,no=2,missing=1,gain=925.15625,cover=156398.203\n",
      "\t1:[CNT_AGR_WO_ARREAR_TO_CNT<0.516649961] yes=3,no=4,missing=3,gain=193.21875,cover=19860.5254\n",
      "\t\t3:leaf=-0.136471063,cover=11818.1885\n",
      "\t\t4:leaf=-0.162643105,cover=8042.33691\n",
      "\t2:[RATE_TR_ALL_L3_6M<0.711987019] yes=5,no=6,missing=5,gain=16.40625,cover=136537.688\n",
      "\t\t5:leaf=-0.173366696,cover=122632.648\n",
      "\t\t6:leaf=-0.161552235,cover=13905.0361\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle \n",
    "import os\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'\n",
    "# save model to file\n",
    "#pickle.dump(gbm_model, open(\"./models/pima.pickle.dat\", \"wb\"))\n",
    "\n",
    "# load model from file\n",
    "#loaded_model = pickle.load(open(\"./models/pima.pickle.dat\", \"rb\"))\n",
    "\n",
    "# dump it to a text file\n",
    "gbm_model.get_booster().dump_model('./models/xgb_model.txt', with_stats=True)\n",
    "# read the contents of the file\n",
    "with open('./models/xgb_model.txt', 'r') as f:\n",
    "    txt_model = f.read()\n",
    "print(txt_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0:[LBT_ACCT_TOT_BAL_PREV_RUB_AMT<30.5649986] yes=1,no=2,missing=1\\n\\t1:[MIN_APP_DAYS<0.5] yes=3,no=4,missing=3\\n\\t\\t3:leaf=-0.145004302\\n\\t\\t4:leaf=-0.174092993\\n\\t2:[CNT_TR_CASH_1M<16.5] yes=5,no=6,missing=5\\n\\t\\t5:leaf=-0.18950969\\n\\t\\t6:leaf=-0.162200376\\n'"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbm_model.get_booster().get_dump()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "b0 = df[(df['LBT_ACCT_TOT_BAL_PREV_RUB_AMT']>=30.5649986)&(df['CNT_TR_CASH_1M']>=16.5)]\n",
    "b1 = df[(df['LBT_ACCT_TOT_BAL_PREV_RUB_AMT']>=64.0350037)&(df['RATE_TR_ALL_L3_6M']>=0.711987019)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4197614810197186"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a0=-0.162200376\n",
    "a1=-0.161552235\n",
    "1/(1+np.exp(-a0-a1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1b0 = pd.merge(b0,b1,left_index=True,right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['y_pred'] = gbm_model.predict_proba(x)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge = pd.merge(df,b1b0,left_index=True,right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88        0.419762\n",
       "313       0.419762\n",
       "356       0.419762\n",
       "649       0.419762\n",
       "1087      0.419762\n",
       "1190      0.419762\n",
       "1253      0.419762\n",
       "1314      0.419762\n",
       "1475      0.419762\n",
       "1583      0.419762\n",
       "1607      0.419762\n",
       "1785      0.419762\n",
       "1862      0.419762\n",
       "2176      0.419762\n",
       "2278      0.419762\n",
       "2481      0.419762\n",
       "2767      0.419762\n",
       "3392      0.419762\n",
       "3480      0.419762\n",
       "3512      0.419762\n",
       "3617      0.419762\n",
       "3635      0.419762\n",
       "3654      0.419762\n",
       "3736      0.419762\n",
       "4102      0.419762\n",
       "4252      0.419762\n",
       "4827      0.419762\n",
       "4998      0.419762\n",
       "5227      0.419762\n",
       "5305      0.419762\n",
       "            ...   \n",
       "591582    0.419762\n",
       "594650    0.419762\n",
       "594683    0.419762\n",
       "596003    0.419762\n",
       "596479    0.419762\n",
       "597516    0.419762\n",
       "600013    0.419762\n",
       "600123    0.419762\n",
       "600731    0.419762\n",
       "600736    0.419762\n",
       "603400    0.419762\n",
       "604557    0.419762\n",
       "605261    0.419762\n",
       "605581    0.419762\n",
       "607465    0.419762\n",
       "608818    0.419762\n",
       "609087    0.419762\n",
       "610967    0.419762\n",
       "611425    0.419762\n",
       "616083    0.419762\n",
       "617879    0.419762\n",
       "617973    0.419762\n",
       "619405    0.419762\n",
       "620761    0.419762\n",
       "621599    0.419762\n",
       "622106    0.419762\n",
       "624103    0.419762\n",
       "626248    0.419762\n",
       "627013    0.419762\n",
       "627519    0.419762\n",
       "Name: y_pred, Length: 2098, dtype: float32"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merge['y_pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17921830314585319"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b1b0['BAD_12_FLAG90_1_y'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random forest classifier\n",
    "start_time = time.time()\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_model = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
    "            max_depth=2, max_features='auto', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=-1,\n",
    "            oob_score=False, random_state=0, verbose=0, warm_start=False)\n",
    "\n",
    "kfold = KFold(n_splits=3, shuffle=True)\n",
    "clf = GridSearchCV(rf_model,{'max_depth':[10],\n",
    "                            'n_estimators':[100]},                             \n",
    "                            scoring='roc_auc',\n",
    "                            cv=kfold,\n",
    "                            n_jobs=-1)\n",
    "clf.fit(x_sample,y)\n",
    "\n",
    "print(-start_time + time.time(),\" sec\")\n",
    "warnings.simplefilter('ignore', FutureWarning)\n",
    "res = pd.DataFrame(clf.cv_results_)\n",
    "res[\"dev_gini\"] = 2*res[\"mean_train_score\"]-1\n",
    "res[\"val_gini\"] = 2*res[\"mean_test_score\"]-1\n",
    "res[[\"params\",\"dev_gini\",\"val_gini\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
